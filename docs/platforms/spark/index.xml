<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Boring Machine Learning</title><link>http://boringml.com/docs/platforms/spark/</link><description>Recent content on Boring Machine Learning</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="http://boringml.com/docs/platforms/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Sample data in PySpark</title><link>http://boringml.com/docs/platforms/spark/create-data-python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/platforms/spark/create-data-python/</guid><description>Here&amp;rsquo;s how to create a small fake dataset for testing in PySpark. More on sc.parallelize.
from pyspark.sql.session import SparkSession rdd = sc.parallelize([(0,None), (0,1), (0,2), (1,2), (1,10), (1,20), (3,18), (3,18), (3,18)]) df=rdd.toDF([&amp;#39;id&amp;#39;,&amp;#39;score&amp;#39;]) df.show() +---+-----+ | id|score| +---+-----+ | 0| null| | 0| 1| | 0| 2| | 1| 2| | 1| 10| | 1| 20| | 3| 18| | 3| 18| | 3| 18| +---+-----+ df.printSchema() root |-- id: long (nullable = true) |-- score: long (nullable = true) None is a special keyword in Python that will let you create nullable fields.</description></item><item><title>Writing Spark files to disk</title><link>http://boringml.com/docs/platforms/spark/writing-files/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/platforms/spark/writing-files/</guid><description>Repartition # Coalesce #</description></item><item><title>Writing Unit Tests for Spark Apps in Scala</title><link>http://boringml.com/docs/platforms/spark/testing-dataframes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://boringml.com/docs/platforms/spark/testing-dataframes/</guid><description>Often, something you’d like to test when you’re writing self-contained Spark applications, is whether your given work on a DataFrame or Dataset will return what you want it to after multiple joins and manipulations to the input data.
This is not different from traditional unit testing, with the only exception that you&amp;rsquo;d like to test and introspect not only the functionality of the code but the data itself.
There’s two ways to be defensive about creating correct data inputs and outputs in Scala Spark.</description></item></channel></rss>